**ðŸ§  Stroke Prediction using Machine Learning**

This project uses supervised machine learning to predict whether a person is likely to suffer a stroke based on medical and demographic features. By applying classification techniques on real-world healthcare data, this project aims to assist in early detection and prevention of strokes.

**ðŸŽ¯ Objective:**

The primary objective is to build a classification model that can accurately predict stroke occurrence. The project walks through a complete machine learning pipelineâ€”from data cleaning and preprocessing to model training, evaluation, and interpretation.

**ðŸ“ Files:**

> Stroke.ipynb â€” Jupyter Notebook with full code, EDA, and modeling

> stroke-data.csv â€” The dataset used for training and evaluation

> README.md â€” Project documentation

**ðŸ“Š Dataset Overview:**

> Source: Kaggle Stroke Prediction Dataset

> Size: ~5,000 records

> Target: stroke (1 = stroke occurred, 0 = no stroke)

**Features:**

> Demographics: gender, age, residence_type

> Health: hypertension, heart_disease, avg_glucose_level, bmi

> Lifestyle: smoking_status, work_type

**ðŸ›  Tools & Technologies:**

> Language: Python 3.x

**Libraries:**

> pandas, numpy â€” Data manipulation

> matplotlib, seaborn â€” Data visualization

> scikit-learn â€” ML models and evaluation

**ðŸ“ˆ ML Pipeline Workflow:**

> Data Exploration and Cleaning

> Handling Missing Values (bmi)

> Label & One-Hot Encoding for Categorical Variables

> Feature Scaling (if needed)

> Train-Test Split

> Model Training (KNN, Naive, SVM, Decision Tree, Random Forest)

> Model Evaluation (Accuracy, Precision, F1-Score, Recall)

> Prediction and Interpretation

**ðŸ“‰ Evaluation Metrics:**

> Accuracy

> Precision & Recall

> F1-Score

> Confusion Matrix

âš ï¸ Note: The dataset is imbalanced, so extra attention is given to Recall and F1-score to minimize false negatives.

**ðŸ” Key Insights:**

> Age, glucose level, and hypertension were the most influential features.

> Ensemble models (e.g., Random Forest, XGBoost) outperformed simple linear models.

> Class balancing and tuning improved minority class (stroke = 1).
